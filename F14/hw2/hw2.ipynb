{
 "metadata": {
  "name": "",
  "signature": "sha256:e916595a43f5a95ddbd07c918fee24aac0ea35485e075a6ae33272ff56e0eaa1"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "CS194-16 Introduction to Data Science\n",
      "\n",
      "**NOTE** click near here to select this cell, esc-Enter will get you into cell edit mode, shift-Enter gets you back\n",
      "\n",
      "**Name**: *Please put your name*\n",
      "\n",
      "**Student ID**: *Please put your student ID*"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "HW2: Parsing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This homework explores the use of synthetic (XML) and natural language parsing for data preparation. It comprises 3 parts:\n",
      "1. Parse some data (Amazon product reviews) in XML to extract the text of the reviews. \n",
      "2. Parse the text reviews into Stanford dependencies using the Stanford Parser, with XML output\n",
      "3. Read the parsed sentences back into Python with the XML parser. \n",
      "\n",
      "We assume you have copied this HW notebook, the stanford parser archive, and the reviews archive into the same directory. Unpack the later two:\n",
      "<pre>\n",
      "tar xvzf reviews.tar.gz\n",
      "tar xvzf stanfordparser.tar.gz\n",
      "</pre>\n",
      "\n",
      "and then copy the parser into /opt:\n",
      "\n",
      "<pre>\n",
      "sudo mv StanfordParser /opt\n",
      "</pre>\n",
      "\n",
      "finally, if you havent already done it, create a personal bin directory:\n",
      "<pre>\n",
      "mkdir ~/bin\n",
      "</pre>\n",
      "\n",
      "scripts or links in that directory will then be in your path. This will be useful for using the Stanford parser (and other tools) later. The path is set in your login script. To make it find the new bin directory you have to log out and log back in again. In the top right hand corner of the VM window you will find a gear-shaped icon. Clicking it yields a drop-down menu with a logout option. Logout, an then back in when you see the login screen. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Part I: XML Parsing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will be using Python's ElementTree API which you can read about here:\n",
      "\n",
      "https://docs.python.org/2/library/xml.etree.elementtree.html\n",
      "\n",
      "Start by loading some XML data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from lxml import etree\n",
      "parser = etree.XMLParser(recover=True)\n",
      "tree = etree.parse('reviews/video/reviews.xml',parser)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "btw, the data is actually far-from-perfect XML. To see some of the defects, remove the argument \"parser\" from the last line, so that it tries instead to parse with a (default) strict parser. You will see it crash at an invalid char string somewhere in the file. You can fix this and find the next problem... But its better to use an auto-recovering parser like the one above. \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> TODO: What kinds of error did you see in the file? (hit esc-Enter to edit this, then ctl-Enter to save)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now lets look at the contents of this tree"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "root=tree.getroot()\n",
      "root"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Nodes have a tag (a name), and possibly attributes (empty in this case)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "root.tag"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "root.attrib"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The children of the root node are accessible using square bracket notation. They will be individual reviews. You can then examine each review node's children by adding additional square bracket fields. Do this now and explore the parse tree. Compare with the file contents (use a text editor to see it). "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "root[2]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You can also use the colon notation to retrieve all the children of a node:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "root[22][:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice that same-named elements can occur multiple times, e.g. unique_id and product_type\n",
      "\n",
      "The \"contents\" of a node are usually held in its text field, which you access like this:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "root[2][0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "root[2][0].text"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we can look at the contents of the other \"unique_id\" node:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "root[2][1].text"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "the find() and findall() methods allow you to find one or (respectively) all the children of a node with a particular tag. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "root[10].find('product_name').text"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "root.findall('review')[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "root[3].find('review_text').text"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Use the ElementTree methods to construct a dataframe containing 11 columns corresponding to the 11 distinct children node types of each review node. Each row should represent a single review. For nodes that may be repeated like \"unique_id\", include a list of the node values in that field."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> TODO: What fraction of the XML review records have two \"unique_id\" nodes? What fraction have two \"product_type\" nodes?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally save the dataFrame as a csv file (you can use a Pandas builtin to do this).\n",
      "\n",
      "For the review text, you should create one file with a unique name per review containing only the review text. The names should be review_text#####.txt where ##### is the number of the review."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Part 2: Natural Language Parsing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the preamble for this HW, you put the Stanford Parser in the /opt directory, and you also created a ~/bin directory. You can use these to put Stanford Parser commands in your path without having to add several new directories to your $PATH variable. There are three commands we will need initially. \n",
      "\n",
      "Open a terminal window and create symlinks like this:\n",
      "\n",
      "<pre>ln -s /opt/StanfordParser/lexparser.sh ~/bin/lexparser.sh\n",
      "\n",
      "ln -s /opt/StanfordParser/lexparser-gui.sh ~/bin/lexparser-gui.sh\n",
      "\n",
      "ln -s /opt/StanfordParser/dependencyviewer/dependencyviewer.sh ~/bin/dependencyviewer.sh</pre>\n",
      "\n",
      "and then type:\n",
      "\n",
      "<pre>\n",
      "lexparser-gui.sh\n",
      "</pre>\n",
      "\n",
      "This brings up a GUI interface to the Stanford parser. To use it, click on \"Load Parser\" which brings up a file selection dialog. Navigate to \n",
      "\n",
      "<pre>/opt/StanfordParser/stanford-parser-3.4.1-models.jar</pre>\n",
      "\n",
      "and open it.\n",
      "\n",
      "Then you will see a list of parsers to use. Select \n",
      "<pre>englishPCFG.ser.gz</pre>\n",
      "\n",
      "You're now ready to parse some text!\n",
      "\n",
      "Click on \"Load File\" and navigate back to your HW2 directory (you'll have to go all the way up to \"/\", and down through \"/home\"). Load your review text file\n",
      "\n",
      "<pre>review_text00000.txt</pre>\n",
      "\n",
      "which will display the text with the first sentence highlighted. Now click on \"Parse\" which will bring up a graphical display of the parsed sentence. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> TODO: Did the sentence parse correctly?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Parse the other sentences from this file. Notice that the yellow highlight is for standard sentences (broken at periods) but that some of these sentences are broken into sentence subparts. \n",
      "\n",
      "This parse tree shows a standard (constituency) tree. Usually we will want to work with dependency trees. To view a dependency tree for the sentences in this file, do \n",
      "\n",
      "<pre>\n",
      "dependencyviewer.sh -in review_text00000.txt\n",
      "</pre>\n",
      "\n",
      "(note the extra \"-in\" option for this parser). This brings up a window with tabs for each of the sentences. click through each sentence and contrast the dependency parse tree with the constituency tree in the other window.\n",
      "\n",
      "Note: Both parsers consume quite a bit of memory so you may need to close the constituency tree viewer before starting the dependency viewer. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> TODO: What are the root nodes for each sentence-like fragment in sentence 5 ? "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The parser also contains scripts for parsing text into structured output. Now run\n",
      "\n",
      "<pre>\n",
      "lexparser.sh review_text00000.txt\n",
      "</pre>\n",
      "\n",
      "You will see both constituency and dependency tree output for each sentence. These formats are ad-hoc though, and not easy for a machine to work with. You can customize the parser startup script. In the main parser directory you will find a script:\n",
      "\n",
      "<pre>\n",
      "/opt/StanfordParser/lexparser.sh\n",
      "</pre>\n",
      "\n",
      "Make your own copy of this script in the same directory, say call it:\n",
      "\n",
      "<pre>\n",
      "/opt/StanfordParser/dependencyparser.sh\n",
      "</pre>\n",
      "\n",
      "This file may not be executable, depending on how you copied it. To make sure it is, do:\n",
      "\n",
      "<pre>\n",
      "chmod 755 dependencyparser.sh\n",
      "</pre>\n",
      " \n",
      "in the Stanford Parser directory. Now open the script in an editor. It contains an invocation of the parser with the option \n",
      "\n",
      "<pre>-outputFormat \"penn,typedDependencies\"</pre>\n",
      "\n",
      "we wont need the penn format output, so you can remove \"penn\" from the options. We need XML output instead of the standard output however. To do that add this option:\n",
      "\n",
      "<pre>\n",
      "-outputFormatOptions \"xml\"\n",
      "</pre>\n",
      "\n",
      "after the -outputFormat option (yes the names are confusing). Save the file. \n",
      "\n",
      "Now from a terminal prompt, create a new symlink from your ~/bin directory to the dependencyparser.sh script. You should now be able to change to the directory containing your sentences and type:\n",
      "\n",
      "<pre>\n",
      "dependencyparser.sh review_text00000.txt\n",
      "</pre>\n",
      "\n",
      "You will see some diagnostic messages, and the XML data. The parser actually sends the XML only to stdout and the diagnostics to stderr. To get just the XML in a file you can do:\n",
      "\n",
      "<pre>\n",
      "dependencyparser.sh review_text00000.txt > review_parsed00000.xml\n",
      "</pre>\n",
      "\n",
      "Now write a bash script (or do in python if you know how to invoke shell commands) to iterate over the input files and produce parsed copies, i.e. by replacing \"00000\" in the filenames above with a series of integer indices. HINT: the bash command for integer iteration is\n",
      "\n",
      "<pre>\n",
      "for i in `seq 0 xxx`\n",
      "do\n",
      "...\n",
      "done\n",
      "</pre> \n",
      "\n",
      "and to get a fixed-length integer string in a file name do:\n",
      "\n",
      "<pre>\n",
      "fname=`printf \"review_text%05d.txt\" $i`\n",
      "</pre>\n",
      "\n",
      "NOTE: Parsing is very time-consuming. You dont have to parse all the reviews, but do at least say the first 100. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> TODO: Give the total of file sizes (e.g. using \"du\" on the directory containing them) for the unparsed text files and the total for the XML parsed files. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Part 3: Reading and Tabulating Targets and Sentiment from Reviews"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Use the ElementTree API to read an XML dependency parse tree from the files that you just created.\n",
      "\n",
      "Write a function to recognize targets and associated sentiment. e.g. a simple pattern is to start at the root node of a dependency tree, look for an nsubj child (a target) and then look for sentiment words - adjectives that attach directly to the subject. Or, look for direct and indirect object words and any adjectives attached to them. \n",
      "\n",
      "More complicated patterns occur when the sentiment words are in a different subtree, rooted by a suitable verb, e.g.\n",
      "\n",
      "\"Lawrence put in a commanding performance...\"\n",
      "\n",
      "Here we would like to extract the subject as the first string in the type, along with the object \"performance\" and its modifier \"commanding\" in the second string. You will need to be careful about the verb however, to make sure that the two subtrees are related. \n",
      "\n",
      "For each pattern that is matched, the function should output the target as a string, and also the sentiment (phrase or list of words) as a string. i.e. in general the output will be a list of (string, string) tuples. Run this function over all the trees from part 2 above. \n",
      "\n",
      "Write one more function that finds a pattern of (target, sentiment words or phrases). This time, define your own pattern by looking through the dependency trees output from part 2. \n",
      "\n",
      "Apply these two functions to each parsed sentence, and concatenate their outputs. Finally concatenate the lists from all sentences. From the final list, construct a dataFrame with \"target\" and \"sentiment\" columns. In the space below cut and paste the first 100 rows of this table (or less if you dont have 100 rows from all the sentences from part 2. \n",
      "\n",
      "Save this notebook and submit it using glookup. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> TODO: Put your analysis code here."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> TODO: Put <=100 rows of your target/sentiment table below:"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}